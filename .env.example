# =======================================================================
# Choose ONE LLM provider below
# Uncomment the option
# =======================================================================

## Option 1: hugging face (remote)
LLM_PROVIDER=hf
LLM_MODEL=openai/gpt-oss-120b
LLM_API_KEY=hf_xxxxxx


## Option 2: openai (remote)
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# LLM_API_KEY=sk-xxxxxx


## Option 3: ollama (local, no API required, but keep it for passing guards)
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1
# LLM_API_KEY=keep_me_here
# OLLAMA_BASE_URL=http://localhost:11434
## (docker url)
# OLLAMA_BASE_URL=http://host.docker.internal:11434

# =======================================================================

## LLM settings (optional)
LLM_MAX_CHUNKS=5
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.3

## Chunks settings
CHUNK_SIZE=200
CHUNK_OVERLAP=50

## Retrieve settings
TOP_K=5
MIN_RETRIEVAL_SCORE=0.18
